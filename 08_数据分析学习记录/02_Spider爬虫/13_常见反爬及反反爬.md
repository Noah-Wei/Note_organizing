# 常见反爬及反反爬

## 一、常见反爬

### 1.1 简介

​	一切限制爬虫程序从服务器获取数据的方式都属于反爬虫。

​	它有多种限制手段，如：限制请求头、限制登陆、验证码检验、限制访问频率等。 从这些限制手段出发，可以将反爬虫分为**主动型**和**被动型**，两种类型。 

​	开发者使用一些技术手段来区分正常用户和爬虫被称为主动型反爬虫，通常有限制请求头、限制访问频率、限制登陆等方式。

​	 为了提高用户体验或节约资源，用一些技术间接提高爬虫访问难度被称为被动型反爬虫，通常有Ajax数据加载、点击切换标签页等方式 

### 1.2 常见反爬

#### 1.2.1 通过headers字段来反爬

​	通过`User-Agen`t字段反爬的话，只需要给他在请求之前添加`User-Agent`即可，更好的方式是使用`User-Agent`池来解决,我们可以考虑收集一堆`User-Agent`的方式，或者是随机生成`User-Agent`

```python
url = 'https://www.baidu.com/'
headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) '
                          'Chrome/119.0.0.0 Safari/537.36'
        }
resp = requests.get(url,headers=headers)
print(resp.status_code)

print(resp.text)
```

#### 1.2.2 通过cookie来反爬

> 如果网站需要**登录**的情况下使用，就需要cookie

- **HTTP**

HTTP：超文本传输协议，客户端向服务器端发送请求需要遵守的协议

HTTP也是一个无状态的协议（无记忆的协议）。 也就是说，HTTP**不会记录**你请求的内容，这样就导致有些状态会丢失。

比如：用户要想进行——点赞操作、评论操作，则用户必须是以登录者的身份进行；进行这些操作之前，用户需要先进行登录（**即发送登录请求，把用户名与密码传递给服务器**）， 进行点赞或评论的操作时知道是你的行为

但是HTTP是无状态的 ，记不住你上一次已经做了登录请求 ，比如点赞就认为你没有登录，拿不到信息，所以出现了cookie（会话技术）

- **Cookie**

cookie：会话技术，接受存储身份标识 ，服务器将身份标识从cookie中返回给客户端；客户端下一次请求的时候从cookie中携带着身份标识给服务器，服务器就通过身份标识判断你是否是登录者

**服务器是把身份标识放在响应头的set_cookie中**

**客户端携带的时候是放在请求头的cookie中**

如果目标网站需要通过登录才能获取信息，则需要设置cookie操作，cookie中保存了用户的登录信息

注意：一个网站一旦登录成功，cookie会保存少则十天半个月，多则长达半年

- **演示**

登录知乎

![image-20231210135428512](https://lskypro-1309218011.cos.ap-shanghai.myqcloud.com/2023/12/10/6575529ce03b7.png)

#### 1.2.3 通过ip地址来反爬

同一个ip大量请求了对方服务器，有更大的可能性会被识别为爬虫，对应的通过购买高质量的ip的方式能够解决；也有免费的代理ip,只是免费的一般都不好用，或者有效期太短

- 测试

频繁的访问豆瓣电影Top250

```python
import requests
from threading import Thread


def get_data():
    url = "https://movie.douban.com/top250"
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'
    }
    resp = requests.get(url, headers=headers)
    print(resp.status_code)


if __name__ == '__main__':
    while True:
        ts = []
        for _ in range(50):
            t = Thread(target=get_data)
            ts.append(t)
            t.start()
        for x in ts:
            x.join()
        print("50次循环结束")
```

使用死循环，50个人不停的访问该网站，一开始状态码都是`200`，随着程序的运行，最后状态码会全部变成`403`

#### 1.2.4 数据加密

这种反爬情况，几乎无破解方法，除非是内部人员，所以建议放弃

## 二、常见反反爬

### 拓展部分

- 动态模版
  - 可以把需要反复使用的代码保存成动态模版，然后通过自己设定的快捷代码触发，节省写代码的时间

- 操作步骤

  - 准备好，作为动态模版的代码，并且复制

    ```python
    import requests
    
    url = ""
    headers = {
        "User-Agent: Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
    }
    resp = requests.get(url, headers=headers)
    if resp.status_code == 200:
        pass
    else:
        pass
    ```

  - 点击Pycharm左上角的`File`——>`Steetings`——>`Editor`——>`Live Templates`——>`Python`

    ![image-20231221193131779](https://lskypro-1309218011.cos.ap-shanghai.myqcloud.com/2023/12/21/6584221be9509.png)

  - 再点击`+`号，选择`Live Templates`

    ![image-20231221193943086](https://lskypro-1309218011.cos.ap-shanghai.myqcloud.com/2023/12/21/658423ff19a55.png)

  - 然后再`Abbrevitaion`中自定义一个快捷代码指令，在`Description`中自定义快捷指令说明，在`Template text`中将代码复制进去

    ![image-20231221194130526](https://lskypro-1309218011.cos.ap-shanghai.myqcloud.com/2023/12/21/6584246a93797.png)

  - 最后在点击`Define`中的`Python`选项勾选，然后点击`Apply`应用

    ![image-20231221194411511](https://lskypro-1309218011.cos.ap-shanghai.myqcloud.com/2023/12/21/6584250b2a940.png)

- 使用

设置好之后，在使用时，我们是需要输入headers就可以直接生成以上的代码

### 2.1 使用cookie

#### 2.1.1 简述

- 自动登录的原理
  -  绝大多数的网站，在浏览器登录成功后，浏览器的cookie会自动保存登录信息
  - 每次用浏览器发送请求，会自动发送cookie中的值完成自动登录

- 爬虫自动登录的方法
  -  发送请求的时候，把已经登录后的cookie发送给服务器
-  **注意**
  - 首先需要人工登录网站，然后在Requests Headers中获取到Cookie,设置给请求头headers

#### 2.1.2 Requests使用Cookie

Requests使用Cookie比较简单，直接登录网站，然后在代码中添加Cookie即可

- 完整代码

```python
import requests

url = "https://www.zhihu.com/"
headers = {
    "Cookie": "_zap=fdfbfc65-d713-4f9e-ba21-89d31446b48a; d_c0=AFCSk9LAsRePTiU9iI_C6XuSARBLr3WxQ2s=|1699845807; YD00517437729195%3AWM_NI=OEGMYxVJd5frGXGMCfY1q4CVbQWr6ySaKAj4e5%2F62Pxy2HH%2FdXTrycyyVp8Te4SDVrPqbJ13oF6nQ6q6%2B83tuN7vnjxoK1Af7uhKfCQXlIyfkwHMfu9FkCzVu%2BAWibX3cTg%3D; YD00517437729195%3AWM_NIKE=9ca17ae2e6ffcda170e2e6eeb3c46af39bb9b8c66fed868ea7c44f838f9badc572ed8ae5a3ec40858f85aab32af0fea7c3b92a8d9b89aedc749696acd6e57a858889aec26e9befbfd5c25ea28f9993b57391bf85a2e940a7ebe1d7b15cbca7fea9cb25a8889c93fc69bba79a91bb60a7ec9da5d57a94afaeabb439ad9afa8fd43eb3eae5d9b280b792f997ed62bcb185d2c46ef5aba8ace5499388a4aafc6ea9a988aee57ffb9aa5a4f95eb7f58fb8ec6a9cedadb5e637e2a3; YD00517437729195%3AWM_TID=LZ2AsGBdTiBEEVQFRQeUmqEqA2oWFOYl; __snaker__id=Y31nH8vzZF35uokP; q_c1=e0491b71e4d6480bafe493a45114d6ba|1699872347000|1699872347000; _xsrf=QSSdbOx1TciSMUj2SSPZ0oNGypYh7XS7; Hm_lvt_98beee57fd2ef70ccdd5ca52b9740c49=1702539893,1702890794,1703153753,1703161282; z_c0=2|1:0|10:1703161282|4:z_c0|80:MS4xWGtWNUNRQUFBQUFtQUFBQVlBSlZUWE1BYUdaczhEVk10WC1SbVVKOUpFTmI1N015WlVFSUVRPT0=|990f2aa443270f1582e7098a98985ccf8e193e9640af88318d55092dc5832f28; SESSIONID=4fCvmf5RWKBQMDiwrnpXvSM7RZtK8BYofDXi0U9QdDo; JOID=UFwUBkrCFTee1zLxNcUQ4xCMHRch8HZw7LlSwWOWWHDLnnWeRwM5s_zSMfA1t6mNrz11UM6VyRj1OayZOtIgb8g=; osd=UV0cBU7DFD-d0zPwPcYU4hGEHhMg8X5z6LhTyWCSWXHDnXGfRgs6t_3TOfMxtqiFrDl0UcaWzRn0Ma-dO9MobMw=; tst=r; Hm_lpvt_98beee57fd2ef70ccdd5ca52b9740c49=1703161289; KLBRSID=031b5396d5ab406499e2ac6fe1bb1a43|1703161298|1703161279",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
}
resp = requests.get(url, headers=headers)
if resp.status_code == 200:
    print(resp.text)
else:
    pass
```

#### 2.1.3 Selenium使用Cookie

相比较Requests使用Cookie，Selenium的方式就比较复杂了，需要分两步走

##### 2.1.3.1 获取Cookie

**注意：在浏览器中手动登录成功之后，因为selenium中无法直接设置cookie，所以通过selenium请求，仍然无法自动登录**

我们首先需要创建浏览器对象，并打开指定网址，然后留出足够的时间，让人工完成登录，这里可以使用一个input函数，打断，保证有足够的时间，登录成功之后，就可以直接调用`get_cookies`方法，将cookies保存到文件中，然后就可以进行下一步的操作了

- 完成代码

```python
import time
from selenium.webdriver import Chrome

# 1.创建浏览器对象，并打开指定网址
url = "https://www.taobao.com/"
browser = Chrome()
browser.maximize_window()
browser.get(url)
time.sleep(2)

# 2.留出足够的时间，让人工完成登录
# 登录成功的标志：再对应窗口中可以看到用户信息
input("是否已完成登录")

# 3.登录成功之后，获取所有的Cookie，并且保存到文件中
cookies = browser.get_cookies()
# print(cookies)
# 保存到本地文件中的Cookie就可以帮助selenium完成登录，或者储存为json形式
with open(r"taobao_cookies.txt", "w", encoding="UTF-8") as f:
    f.write(str(cookies))
```

##### 2.1.3.2 使用Cookie

在上一步中，我们将获取到的Cookies保存在一个文件中，所以这次我们直接从文本中获取cookies，然后调用`add_cookie`方法，将cookie添加到浏览器对象中，在重新打开网页即可

- 完整代码

```python
import time
from selenium.webdriver import Chrome

# 1.创建浏览器打开需要自动登录的网页
browser = Chrome()
browser.maximize_window()
url = "https://www.taobao.com/"
browser.get(url)

# 2.从文本中获取Cookie
with open(r"taobao_cookies.txt", "r", encoding="UTF_8") as f:
    cookies = eval(f.read())

# 3.添加cookie
for cookie in cookies:
    browser.add_cookie(cookie)

time.sleep(3)

# 4.重新打开网页
browser.get(url)

input("end:")
```

##### 注意事项和总结

- 当selenium自动打开浏览器之后，需要手动登录网站
- 手动登录成功之后，该窗口中会显示登录信息
- selenium每次只要重新运行程序，都需要重新登录
- 获取cookie的程序可能很久才需要运行一次，只要获取到cookie，就可以直接使用

### 2.2 ip代理的使用

> 可选择免费的IP代理，或者付费IP代理（极光代理/芝麻代理）
>
> 注意：这些不同的平台，都需要进行实名认证,而且第一次提取IP时，需要签订合同
>
> 步骤：
>
> ​	1.注册账号，并登录
>
> ​	2.实名认证
>
> ​	3.签订合同

#### 2.2.1 Request设置代理

```python
# http://webapi.http.zhimacangku.com/getip?num=2&type=2&pro=110000&city=110200&yys=0&port=11&time=1&ts=0&ys=0&cs=0&lb=1&sb=0&pb=4&mr=1&regions=

import  requests,re

def get_data():
 proxy_url = 'http://webapi.http.zhimacangku.com/getip?num=4&type=2&pro=110000&city=110200&yys=0&port=1&time=1&ts=0&ys=0&cs=0&lb=1&sb=0&pb=4&mr=1&regions='
 resp = requests.get(proxy_url)
 # 获取请求代理ip之后的json数据
 result = resp.json()
 print(result) # {'code': 0, 'data': [{'ip': '114.245.110.186', 'port': 4225}, {'ip': '114.245.109.200', 'port': 4225}], 'msg': '0', 'success': True}
 print(f'http://{result["data"][2]["ip"]}:{result["data"][2]["port"]}')
 url = 'https://www.maoyan.com/films?showType=3'
 headers = {
     'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36'
 }
 # http://ip地址:端口
 proxies = {
     'http':f'http://{result["data"][2]["ip"]}:{result["data"][2]["port"]}'
 }
 resp = requests.get(url,headers=headers,proxies=proxies)
 if resp.status_code == 200:
     print('请求成功！')
     resp.encoding = 'utf-8'
     print(resp.text)
     parse_data(resp.text)
 else:
     print('请求失败', resp.status_code)
# 2.解析数据
def parse_data(data):
 # 电影名称
 '''
 <div class="channel-detail movie-item-title" title="前任4：英年早婚">
   <a href="/films/1405256" target="_blank" data-act="movies-click" data-val="{movieId:1405256}">前任4：英年早婚</a>
 '''
 regex_name = r'<div class="channel-detail movie-item-title".*?>.*?<a.*?>(.+?)</a>'
 movie_names = re.findall(regex_name,data,flags=re.DOTALL)
 print(movie_names)

 # 评分
 '''
 有评分
 <div class="channel-detail channel-detail-orange"><i class="integer">9.</i><i class="fraction">0</i></div>
 无评分
 <div class="channel-detail channel-detail-orange">暂无评分</div>
 '''
 regex_score = r'<div class="channel-detail channel-detail-orange"><i class="integer">(.+?)</i><i class="fraction">(.+?)</i></div>'
 movie_scores = re.findall(regex_score,data)
 print(movie_scores)

if __name__ == '__main__':
 get_data()
```

#### 2.2.2 Selenium设置代理

```python
import requests
from  selenium.webdriver import  Chrome
from selenium.webdriver.chrome.options import Options

proxy_url = 'http://webapi.http.zhimacangku.com/getip?num=4&type=2&pro=110000&city=110200&yys=0&port=1&time=1&ts=0&ys=0&cs=0&lb=1&sb=0&pb=4&mr=1&regions='
resp = requests.get(proxy_url)
# 获取请求代理ip之后的json数据
result = resp.json()
print(result)

# 设置代理ip和端口
proxy_ip = result['data'][0]['ip']
proxy_port = result['data'][0]['port']

# 创建chrome选项
options = Options()
options.add_argument(f'--proxy-server={proxy_ip}:{proxy_port}')

url = 'https://www.maoyan.com/films?showType=3'
b = Chrome()
b.get(url)

input('end:')
```

